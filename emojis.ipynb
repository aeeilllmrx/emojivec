{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A shallow dive into word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've been tinkering with word2vec for a bunch of projects now, and I thought I'd give a quick demo of how it can be used on a tiny amount of Twitter data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the data you want, you'll want to know your consumer keys and access token keys. There are a few ways to proceed, but I found it simplest to use the \"twitter\" package, a very basic API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "\n",
    "# set these with your own values\n",
    "\n",
    "api = twitter.Api(consumer_key=con_key,\n",
    "consumer_secret=con_secret,\n",
    "access_token_key=token_key,\n",
    "access_token_secret=token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we log in, it's as simple as grabbing all the posts in your timeline and putting them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "users = api.GetFriends()\n",
    "users = [u.name for u in users]\n",
    "\n",
    "tweets = []\n",
    "for u in users:\n",
    "    posts = api.GetUserTimeline(screen_name=user)\n",
    "    tweets.extend(posts)\n",
    "tweets = [t.text for t in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of word2vec is that it takes a word and transforms it into a vector. If this is done right, you can use these vectors to see how similar words are (with various distance metrics), cluster words, and even figure out their meanings. \n",
    "\n",
    "A famous example of this is that for a well-trained data set, the vector for 'king' minus the vector for 'man', plus the vector for 'woman', should be very close to the vector for 'queen'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here, I thought it would be nice to try to figure out the meanings of emoji, as they are all the rage on Twitter.\n",
    "\n",
    "With that in mind, I used some regular expressions extract all emoji from our Twitter data, using the fact that they all have a particular unicode representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ðŸ”ª    17\n",
       "ðŸ˜ˆ    17\n",
       "ðŸ¥    17\n",
       "ðŸ™Œ    17\n",
       "ðŸ˜ƒ    17\n",
       "ðŸ˜¡    17\n",
       "ðŸ§    17\n",
       "ðŸŽ§    17\n",
       "ðŸ“…    17\n",
       "ðŸŒ»    17\n",
       "ðŸ––    17\n",
       "dtype: int64"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re  \n",
    "import pandas as pd\n",
    "\n",
    "try:  \n",
    "    # UCS-4\n",
    "    e = re.compile(u'[\\U00010000-\\U0010ffff]')\n",
    "except re.error:  \n",
    "    # UCS-2\n",
    "    e = re.compile(u'[\\uD800-\\uDBFF][\\uDC00-\\uDFFF]')\n",
    "\n",
    "emoji = []  \n",
    "for x in tweets:  \n",
    "    match  = e.search(x)\n",
    "    if match:\n",
    "        emoji.append(match.group())\n",
    "\n",
    "# counting up how much of each emoji we have\n",
    "df =  pd.Series('emoji': emoji) \n",
    "df = df.groupby(df['emoji'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we don't have too many examples, but let's see what happens anyway. We train the word2vec model on the tweets that contain emoji, trying to learn what syntactic relationships exist between the emoji and surrounding words.\n",
    "\n",
    "Our parameters include window (how many words forward and back to include as meaningful), size (how big the vector representation is), and min_count (how many times a word must appear for it to be deemed relevant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "words = [i.split() for i in tweets]\n",
    "model = word2vec.Word2Vec(words, size=100, min_count = 5,\n",
    "            window = 10, sample = 1e-2)\n",
    "\n",
    "model.init_sims(replace=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work? Well, let's take a look at an example. Here we see that for the calendar emoji, our model identifies \"calendar\" as the most similar word, followed by much, confusion, July, and last. Not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“…\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'calendar', 0.993647038936615),\n",
       " (u'Much', 0.9935946464538574),\n",
       " (u'confusion', 0.9879370927810669),\n",
       " (u'July', 0.9805886745452881),\n",
       " (u'last', 0.9793851971626282)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print u'\\U0001f4c5'\n",
    "\n",
    "model.most_similar(u'\\U0001f4c5',topn=5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Finally, even though we don't have that much data, let's take a look at some other emoji correlations. Amid some non-sensical connections, we can see that the headphones emoji is paired up with \"Happy\", the \"raising both hands\" emoji is paired with \"Greatest\", and the \"knife\" emoji is paired with tsunami.\n",
    "\n",
    "I encourage you to try this with your own Twitter feed and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¥ ðŸ˜\n",
      "ðŸ¥ ðŸ«\n",
      "ðŸ¥ ðŸ¦\n",
      "ðŸ–– ðŸž\n",
      "ðŸ–– ðŸœ\n",
      "ðŸ–– ðŸ—»\n",
      "need more data\n",
      "ðŸ“… calendar\n",
      "ðŸ“… Much\n",
      "ðŸ“… confusion\n",
      "ðŸŽ§ http://t.co/e1FQqu4sb1\n",
      "ðŸŽ§ Happy\n",
      "ðŸŽ§ ðŸ¢\n",
      "ðŸ§ a\n",
      "ðŸ˜¡ THE\n",
      "ðŸ˜¡ emoji\n",
      "ðŸ˜¡ only\n",
      "need more data\n",
      "ðŸ™Œ dayâ€¦ever?\n",
      "ðŸ™Œ Greatest\n",
      "ðŸ™Œ https://t.co/1mrqYuDMcv\n",
      "need more data\n",
      "ðŸ”ª 1\n",
      "ðŸ”ª tsunami;\n",
      "ðŸ”ª tsunamis\n"
     ]
    }
   ],
   "source": [
    "s = list(set(emojis))\n",
    "for i in s:\n",
    "    try:\n",
    "        candidates = model.most_similar(i,topn=3)\n",
    "        for j in candidates:\n",
    "            try:\n",
    "                if model.similarity(i, j[0]) > 0.8:\n",
    "                    print i,j[0]\n",
    "            except:\n",
    "                print \"?\"\n",
    "                continue\n",
    "    except KeyError:\n",
    "        print \"need more data\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
